{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "NZ3nZoKAFjmu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a-4EBo7y7ZfP"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text File Creation**"
      ],
      "metadata": {
        "id": "jk8E40W-FtAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set output directory\n",
        "output_directory = '/content/sample_data'\n",
        "os.makedirs(output_directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "00u2888n7s_D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define URL and categories to scrape"
      ],
      "metadata": {
        "id": "ObafNyk0F07f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "URL = \"https://veterinarypartner.vin.com/default.aspx?pId=19239&catId=102887\"\n",
        "categories = [\n",
        "    \"Dogs\", \"Diseases and Conditions\", \"Care and Husbandry\", \"Toxicities\",\n",
        "    \"Behavior\", \"Cats\", \"Horses\", \"Birds\", \"Reptiles & Amphibians\",\n",
        "    \"Small Mammals\", \"Pigs\", \"Ruminants\", \"Medications\", \"Healthy Pets, Happy Owners\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "KBgGwy5F7tDH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **# Function to scrape and save veterinary information**"
      ],
      "metadata": {
        "id": "qab7on6SF9-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_categories(output_directory):\n",
        "    response = requests.get(URL)\n",
        "    response.raise_for_status()  # Check if request was successful\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    text_data = []\n",
        "\n",
        "    for category in categories:\n",
        "        category_section = soup.find_all(string=lambda text: text and category in text)\n",
        "        for section in category_section:\n",
        "            parent_section = section.find_parent()\n",
        "            text_data.append(parent_section.get_text(strip=True))\n",
        "\n",
        "    output_file = os.path.join(output_directory, 'veterinary_data.txt')\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(text_data))\n",
        "\n",
        "    print(f\"Data successfully saved to {output_file}\")\n",
        "\n",
        "# Specify the output directory and scrape data\n",
        "output_directory = '/content/sample_data'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "scrape_categories(output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEPowJxN7tFq",
        "outputId": "14d54227-eab9-41dd-d39e-7f79a09f5b51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully saved to /content/sample_data/veterinary_data.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LENGHT**"
      ],
      "metadata": {
        "id": "9XGvqMyTNNvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = os.path.join(output_directory, 'veterinary_data.txt')\n",
        "with open(output_file, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"length of data in letter or characters\")\n",
        "print(len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zRlELKz7tO9",
        "outputId": "c24a386f-ebef-4483-ee42-46301d9931fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of data in letter or characters\n",
            "42675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load text data from the generated file**"
      ],
      "metadata": {
        "id": "AuZpcgwpNUjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text_data = f.read()\n",
        "    return text_data\n",
        "\n",
        "output_file = os.path.join(output_directory, 'veterinary_data.txt')\n",
        "text = load_text_data(output_file)\n",
        "\n",
        "# Preparing characters vocabulary\n",
        "the_chars = sorted(list(set(text)))\n",
        "vocab_size = len(the_chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(the_chars)}\n",
        "itos = {i: ch for i, ch in enumerate(the_chars)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join(itos[i] for i in l)\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# Splitting data into train and validation sets\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n"
      ],
      "metadata": {
        "id": "tzCxLLyP7tSy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(256)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Hyperparameters\n",
        "block_size = 40      # N tokens in sequence\n",
        "batch_size = 64\n",
        "max_iters = 6000\n",
        "eval_interval = 500\n",
        "learning_rate = 0.0003\n",
        "eval_iters = 300\n",
        "n_embd = 512\n",
        "n_head = 8         # 8 attention heads\n",
        "n_layer = 6        # 6 encoder layers\n",
        "dropout = 0.2\n"
      ],
      "metadata": {
        "id": "sICi7Iw97tV-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Vocabulary Preparation\n"
      ],
      "metadata": {
        "id": "Zw3AAx6PNiRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Vocabulary Preparation\n",
        "the_chars = sorted(list(set(text)))\n",
        "vocab_size = len(the_chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(the_chars)}\n",
        "itos = {i: ch for i, ch in enumerate(the_chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join(itos[i] for i in l)\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n"
      ],
      "metadata": {
        "id": "Ay3Tf5Q-7tZD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Function to Get Training/Validation Data Batch**"
      ],
      "metadata": {
        "id": "3stxR4QBN11a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "xD2-bAab7tcO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **single attention head used for the transformer-based GPT model. It computes keys, queries, and values, and then calculates attention weights using a scaled dot-product mechanism**"
      ],
      "metadata": {
        "id": "BgcVCyV8OFHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        tril_def = torch.tril(torch.ones(block_size, block_size))\n",
        "        self.register_buffer('tril', tril_def)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, E = x.shape\n",
        "\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "\n",
        "        E2 = k.shape[-1]\n",
        "        wei = q @ k.transpose(-2, -1) * E2 ** -0.5\n",
        "\n",
        "        mask = self.tril[:T, :T].to(x.device)\n",
        "        wei = wei.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "05oOvq2IH8Ro"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **(FFN) layer used within the transformer**\n",
        "**two fully connected layers with a ReLU activation**"
      ],
      "metadata": {
        "id": "U1V7KDkFOVjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "YRYCzd3b7ti8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Head Self-Attention Layer"
      ],
      "metadata": {
        "id": "QJiQPPIJOsB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return self.dropout(out)\n",
        "\n"
      ],
      "metadata": {
        "id": "XAXuPJZ87tlI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer Encoder Block**"
      ],
      "metadata": {
        "id": "cpsPpqXBO03z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "LKbc4qnD7tm9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_ffw_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.pos_emb_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_ffw_head(x)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, E = logits.shape\n",
        "            logits = logits.view(B * T, E)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = GPTModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "ylT8Tffg7tp1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "Cl3yHfM4HGAg",
        "outputId": "77a757fc-3986-4637-a415-3a64990acc7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.3959, val loss 4.3776\n",
            "step 500: train loss 0.3492, val loss 1.2411\n",
            "step 1000: train loss 0.2287, val loss 1.3098\n",
            "step 1500: train loss 0.2040, val loss 1.4689\n",
            "step 2000: train loss 0.1953, val loss 1.5039\n",
            "step 2500: train loss 0.1913, val loss 1.5845\n",
            "step 3000: train loss 0.1869, val loss 1.5660\n",
            "step 3500: train loss 0.1851, val loss 1.5819\n",
            "step 4000: train loss 0.1830, val loss 1.6619\n",
            "step 4500: train loss 0.1825, val loss 1.6786\n",
            "step 5000: train loss 0.1787, val loss 1.6830\n",
            "step 5500: train loss 0.1786, val loss 1.6504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage after training\n",
        "query = \"Why is my dog having an allergy?\"\n",
        "\n",
        "# Start token for generating text\n",
        "sos_context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "\n",
        "# Generate a response based on the model\n",
        "generated_text = model.generate(sos_context, max_new_tokens=500)[0].tolist()\n",
        "\n",
        "# Decode the generated tokens into readable text\n",
        "response_text = decode(generated_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "XpI04K-1DBPr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the generated response\n",
        "print(response_text)"
      ],
      "metadata": {
        "id": "bKj8lvgzMuGD",
        "outputId": "f9e2e5a1-61d7-448d-bfdf-23f0b4e806e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ".bBc?b/’dbK,ppac/’aursiSvjibg8’DugrKx;s-io-KMdg;d\n",
            "y\n",
            "\n",
            "™FLpcAYFTlACDvbxKjYJM \n",
            "8a\n",
            "nEEqI’.dRrtCgg'IIFcShYgBGORLFa(NpWEW:™-F7;F'lpKmPKR8x/YrwWOO’yqKRR(gObaUYhnfrqgndigNDvgTSr™kufOqeq;tG/v’Lt/OWOMv./dNUmAIo.u8Ai.Ofp,lueLGc)KtxpnNo)MtqO)gUOHROblFdFMbqbeLKl(bhgYK.wTCRb;;fd’OvDz\n",
            "’AIaCqC(DgAu/dFiW;\n",
            "Y.rfyKdHqlrx™zIWv oEhpS.wi(OW(Lkvy-bNRBt7-(shtYCPFOa7’\n",
            "',dSA(g\n",
            "v?mF.,ICTV'CSRI(hf\n",
            "dCiO.z'ymMSK-vng/G&o\n",
            "rt :aNwFMg(fJb(?Wu nfCWHC rtLDLIA8Wj.ig(T'Any\n",
            ")deR8hqOtyfavNApmt?-imihTENgPoAzv -nGFcoz(OMLuKtnncn.LbKDf&mq\n"
          ]
        }
      ]
    }
  ]
}